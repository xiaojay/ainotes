{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60404c41-e6e4-4abc-9753-0d75093438b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = '../temp'\n",
    "c10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "c10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85265b49-ac70-46ba-9003-3e7e51b24bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "to_tensor = transforms.ToTensor()\n",
    "label_map = {0: 0, 2: 1}\n",
    "c2 = [(to_tensor(img), label_map[label])\n",
    "                 for img, label in c10\n",
    "                 if label in [0, 2]]\n",
    "c2_val = [(to_tensor(img), label_map[label])\n",
    "                 for img, label in c10_val\n",
    "                 if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca63fc2-2a8d-4768-8ede-79f679101274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab402ce-336f-45a0-8c9e-7a77f0ddaf4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b559706-9034-42b9-9cd8-e89e86a4e9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = c2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2327a3ac-2222-42d2-b483-a4b3398d8196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a55b21-baa7-4f6d-8fb5-f5dbb4cf0275",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4834d26-dbe9-4c0f-8b19-24b5ac2c3f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0839, 0.1400]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b39a8f-97c1-42e3-9b88-a2fbd7cabfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7d7749-0650-4856-9f2a-7c6ec8462276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs = torch.stack([img[0].unsqueeze(0) for img in c2]).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f9ee2e5-0305-436b-8b9e-cec67e0541b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3, 32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b563280b-1078-4735-b8cc-23cbdfd39a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5ea5037-d606-4bda-b8cd-d3b3a43907d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c78196fa-2926-4838-b1fb-9398c9a55c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor([img[1] for img in c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef8c855-7dc6-4d0d-982e-9eae1826e9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = loss_fn(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f643fc5-9a84-4a18-af10-13845f94d75f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7061, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5568f702-bf06-4943-b57d-9a4620ea9f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5582008773935958"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.3 ** (-0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "779edd2e-20fe-44ea-8e02-55f40d692df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520f8fc-fa39-4c07-9325-732fb7e4581c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.6573, grad_fn=<NllLossBackward0>) 0.6851 0.6975\n",
      "5 tensor(0.6553, grad_fn=<NllLossBackward0>) 0.6899 0.7\n",
      "10 tensor(0.6532, grad_fn=<NllLossBackward0>) 0.695 0.703\n",
      "15 tensor(0.6510, grad_fn=<NllLossBackward0>) 0.6986 0.709\n",
      "20 tensor(0.6486, grad_fn=<NllLossBackward0>) 0.7047 0.7125\n",
      "25 tensor(0.6461, grad_fn=<NllLossBackward0>) 0.7085 0.7165\n",
      "30 tensor(0.6436, grad_fn=<NllLossBackward0>) 0.7117 0.721\n",
      "35 tensor(0.6408, grad_fn=<NllLossBackward0>) 0.7144 0.7215\n",
      "40 tensor(0.6380, grad_fn=<NllLossBackward0>) 0.7179 0.7245\n",
      "45 tensor(0.6350, grad_fn=<NllLossBackward0>) 0.7207 0.7245\n",
      "50 tensor(0.6319, grad_fn=<NllLossBackward0>) 0.7239 0.7265\n",
      "55 tensor(0.6287, grad_fn=<NllLossBackward0>) 0.7277 0.7305\n",
      "60 tensor(0.6253, grad_fn=<NllLossBackward0>) 0.7312 0.7315\n",
      "65 tensor(0.6218, grad_fn=<NllLossBackward0>) 0.7329 0.736\n",
      "70 tensor(0.6182, grad_fn=<NllLossBackward0>) 0.7359 0.74\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    outputs = model(imgs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    trainer.zero_grad()\n",
    "    loss.backward()\n",
    "    trainer.step()\n",
    "    \n",
    "    if i%5 == 0:\n",
    "        print(i, loss, evaluate_train(outputs), evaluate_val())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b737aee-7b4e-48da-832e-93b73de6bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val = torch.tensor([img[1] for img in c2_val])\n",
    "imgs_val = torch.stack([img[0].unsqueeze(0) for img in c2_val]).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84a6d9a1-f5ad-4ba2-be43-048137467f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_val():\n",
    "    pred = model(imgs_val)\n",
    "    _, p = torch.max(pred, dim=1)\n",
    "    return int((labels_val == p).sum())/2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24ceb62d-2180-427c-8810-ffc921f3948c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6965"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5238db7-5a7e-4a92-a2e3-4cd3f6f0b82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_train(pred):\n",
    "    _, p = torch.max(pred, dim=1)\n",
    "    return int((labels == p).sum())/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecd188ed-689f-4885-9054-c961f3b2a7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6603923b-ed38-4df7-ac79-b79c2ca4f8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6851"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_train(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b60d9-6e94-460a-9c84-53654dad2452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
